Overview:
You are receiving on the order of 1bn executions daily after the regular trading session ends at 4pm ET. These client executions arrive in the form of compressed files that contain one raw FIX message per line. The files are provided by 10 different vendors, each of which are responsible for sending you executions for orders done on different trading platforms.

Goal:
Your users want to create a client analytics platform that can leverage daily execution data to identify trends over time.

Prompt:
Design a data ingestion and analytics pipeline that can identify and present trends over time.

Example FIX message:

8=FIX.4.2|11=ABC-0001|35=8|44=170.1|52=20240501-09:30:01.500|53=100|54=1|55=AAPL|109=1001

8: BeginString (Identifies beginning of message, including protocol version)
11: ClOrdID (Client Order ID)
35: MsgType (8=Execution Report)
44: Price
52: TransactTime (Time the execution occurred)
53: Quantity
54: Side (1=Buy, 2=Sell)
55: Symbol
109: ClientID (Unique internal identifier for a client)





Questions:
1. Data Sources & Ingestion
How are the FIX messages delivered? Are they sent as compressed files (e.g., .gz, .zip) via SFTP, HTTP, or other methods?
Are the FIX messages consistent across vendors, or do we need to normalize/standardize them during ingestion?
How many lines (FIX messages) does each file contain, and what is the approximate size of a file?
How frequently do the files arrive? Do we expect these files to arrive all at once, or are they sent in real-time/periodically throughout the day?
Do we need to handle failures during data ingestion? What should happen if a file is corrupted or if a vendor fails to send their data?
2. Data Processing & Transformation
What specific fields from the FIX messages are important for analysis?
Should we use the exact timestamp from the FIX message for historical analysis? How should we handle timezones or normalize timestamps for reporting? Should we aggregate at a minute, hourly, or daily level?
What are the performance SLAs for data processing? Do we need real-time or batch processing? What is the acceptable processing window?
Should any additional data enrichment (e.g., price adjustments, metadata) be applied to the FIX messages before storing them?
Do we need to cleanse or validate the data (e.g., filtering out invalid trades, handling missing or out-of-bounds values)?
3. Analytics & Trends
What key metrics (e.g., moving averages, volume-weighted average price) need to be computed?
Should the system support full historical trend analysis (e.g., over months or years), or will it only track trends within a fixed window (e.g., the last 30 days)?
Do we need to identify anomalies (e.g., unusual price movements, spikes in trade volume)? Should the system trigger alerts when trends deviate significantly from expected patterns?

5. Error Handling & Monitoring
What happens if a message or file is incomplete or malformed? Should we reject, ignore, or log these errors? Should we trigger alerts when such errors occur?
8. Integration with Other Systems
Do we need to expose APIs for external access? Should external systems or clients be able to query the analytics platform? Should we provide REST APIs or GraphQL for data access?




1. Data Ingestion

Vendors send data in compressed files (or as messages), and these messages are immediately pushed to the broker. The broker ensures that data is stored temporarily in a fault-tolerant manner until the downstream services (like Spark or Flink) can process it.
Vendors push execution files containing FIX messages to the broker.
The broker stores these messages and allows the processing layer (e.g., Apache Flink, Spark, or Lambda functions) to pull data and process it asynchronously.
If any failures occur, the data remains in the broker until the issue is resolved, ensuring no data loss.

Example Scenario: Using Kafka for Large File Metadata and S3 for Storage
Let's consider a scenario where vendors send large execution data files (e.g., FIX logs) for processing:
Step 1: Vendor uploads file to S3:
The vendor uses SFTP or an API to upload a large file (e.g., a 1 GB file) to S3.
Step 2: Send metadata to Kafka:
The vendorâ€™s system sends a Kafka message containing metadata about the uploaded file (e.g., file name, size, S3 path, upload timestamp).
Step 3: Consumer listens to Kafka:
A consumer service listens to the Kafka topic and reads the metadata for the newly uploaded file.
Step 4: Consumer fetches file from S3:
The consumer uses the file path from the Kafka message to fetch the actual file from S3 and begin processing it.


2. Data Parsing & Transformation
The raw FIX messages need to be parsed and transformed into a structured format. Each FIX message contains relevant fields like ClOrdID, Price, Quantity, Symbol, etc., that need to be extracted for further analysis.

When record fields are changeable, Use a Flexible Data Format: JSON or Parquet are commonly used for handling changing fields because they allow for dynamic and semi-structured data.


3. Data Storage
For easy querying and aggregation, store the transformed data in a structured format like Parquet or ORC in cloud storage (S3) or distributed databases (HDFS)

4. Analytics & Query Layer
Data Aggregation: Create scheduled jobs (using Apache Airflow or AWS Step Functions) to aggregate the data at the necessary granularity (e.g., by hour, day, or week).
Data Access Layer: Expose the data to the analytics team or client-facing platforms via an API or query interface. This could be a RESTful API powered by GraphQL or SQL-based queries.
Reporting and Dashboards: Build reports and dashboards to display trend analysis (price movements, trading volumes, etc.) over different time frames.
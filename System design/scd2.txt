"""
Build a service that sends a daily email containing plots that show stock price trends.

Data Source:
We receive daily stock price data from a 3rd party that delivers a CSV file at the daily grain.

Initial Data Delivery (Full History):

Delivery Date = 2023-04-01 (data backfilled to 2020)

Example of Full History Table:
| ticker    | Closing Val | Timestamp  |
|-----------|-------------|------------|
| Ticker-1  | 95          | 2023-01-01 |
| Ticker-2  | 100         | 2023-01-01 |
| Ticker-3  | 105         | 2023-01-01 |
| ...       | ...         | ...        |
| Ticker-1  | 100         | 2023-04-01 |
| Ticker-2  | 105         | 2023-04-01 |
| Ticker-3  | 1110        | 2023-04-01 |
| Ticker-Bad1 | -50       | 2023-04-01 |

Incremental Data Delivery (Daily Updates):

Delivery Date = 2023-04-02

Example of Daily Incremental Table:
| ticker    | Closing Val | Timestamp  | Action |
|-----------|-------------|------------|--------|
| Ticker-1  | 95          | 2023-04-02 | I      |
| Ticker-2  | 100         | 2023-04-02 | I      |
| Ticker-3  | 105         | 2023-04-02 | I      |
| Ticker-3  | 114         | 2023-04-01 | U     |
| Ticker-Bad1 | ...       | ...        | D      |

Incremental Data Actions:
- 'I' indicates that the record should be inserted into the full history table.
  If the record already exists, the new incremental value should overwrite it.
- 'D' indicates that the record should be deleted from the full history table and not inserted.

Tasks:

1) Architect a pipeline, including code (if needed), to ingest this data.
# identify data source - email
# build process download 

s3://ticker-data/{file_date}_history.csv@
s3://ticker-data/{file_date}_incremental.csv

# extract 4pm
df = SparkSession(..).read(s3://)

# transform
| ticker    | Closing Val | Timestamp  | Action |valid_to| valid_from
|-----------|-------------|------------|--------|
| Ticker-3  | 1110         | 2023-04-01 | I     |2023-04-02: 16:00|2023-04-01 16:00|

 | Ticker-3  | 114         | 2023-04-01 | I     |9999-04-02: 16:00|2


s3://ticker-data/team1/ticker1.csv: team@point72.com

s3://ticker-data/ticker_1_plot.html
s3://ticker-data/ticker_2_plot.html



# Load (30days)


2) Build a service to email users plots of stock price trends as soon as the data is refreshed,
   based on what the users subscribe to.
"""







1.Before diving into the architecture and implementation, it's essential to understand the data and the required flow:
Key Questions to Raise:
*How do we ensure the integrity of the data being delivered?
We need to consider how to handle potential issues like duplicate data, missing data, or invalid records. What are the mechanisms for data validation?

*What is the frequency of data updates?
Is the CSV file delivered at a fixed time each day (e.g., every 12 AM UTC)? Or can it vary? Will the data always be provided in the same format and structure?

*How large is the dataset?
What is the expected size of the data (number of tickers, daily price entries, etc.)? This will impact the choice of storage, processing, and performance considerations.
How do we handle error scenarios?

*What happens if the data delivery fails? How do we notify the system administrators, and how do we retry the process?

*How should we handle bad data?
How should we handle erroneous data (e.g., negative stock prices or invalid tickers)? Should we discard or flag such entries?

*Do we need real-time updates, or is a daily refresh acceptable?
Are users expected to see trends immediately after new data is ingested, or is it acceptable to batch-process data once every 24 hours?

2. High-Level System Design
Components:
Data Ingestion Layer: This component is responsible for receiving the CSV file (both initial history and daily updates), validating it, and storing it in a database.
Data Storage: A relational database (SQL) or NoSQL database will store the stock prices and relevant metadata (e.g., ticker, closing value, timestamp).
Data Processing: This component processes the daily updates to update the stock price data and handle actions like inserting new data, updating existing records, and deleting obsolete records.
Email Generation and Subscription Management: Once the data is processed, the service should generate and send emails with stock price trends as per the user's subscriptions.


3. Data Ingestion & Storage
Ingestion Pipeline
Initial Data Delivery:

When the full history is delivered (e.g., April 1st), ingest all data and load it into a database.
The data should be inserted into the table and timestamped for accurate tracking.
Incremental Data Delivery:

Daily incremental updates (e.g., April 2nd) include:
Insert new records (I action) and update existing records.
Delete records (D action) if a stock ticker is invalid or no longer relevant.
Data Storage Considerations:
Schema Design:

Use a stocks table that records the stock price for each ticker by date, with the following columns:
ticker (Primary key)
closing_value
timestamp
action (for incremental updates)
Example table:

sql
Copy code
CREATE TABLE stock_prices (
  ticker VARCHAR(255),
  closing_value DECIMAL(10, 2),
  timestamp DATE,
  PRIMARY KEY (ticker, timestamp)
);
Handling the Actions (I, U, D):

When processing the incremental data:
Insert (I): If the record is new or needs to be updated, insert or update the record in the database.
Update (U): Modify the closing value of an existing stock price for a particular date.
Delete (D): Remove invalid stock price data.
Solution (Python Example for Data Processing):
Here is a Python script using SQLAlchemy to insert and update the stock data.

python
Copy code
from sqlalchemy import create_engine, Column, Integer, String, Date, DECIMAL
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

Base = declarative_base()

class StockPrice(Base):
    __tablename__ = 'stock_prices'
    ticker = Column(String, primary_key=True)
    closing_value = Column(DECIMAL)
    timestamp = Column(Date, primary_key=True)

# Database connection
engine = create_engine('sqlite:///stocks.db')  # Replace with your DB connection
Base.metadata.create_all(engine)
Session = sessionmaker(bind=engine)
session = Session()

def process_incremental_data(data):
    for row in data:
        ticker = row['ticker']
        closing_value = row['closing_value']
        timestamp = row['timestamp']
        action = row['action']

        if action == 'I':  # Insert or Update
            stock = session.query(StockPrice).filter_by(ticker=ticker, timestamp=timestamp).first()
            if stock:
                stock.closing_value = closing_value  # Update existing record
            else:
                stock = StockPrice(ticker=ticker, closing_value=closing_value, timestamp=timestamp)
                session.add(stock)
        
        elif action == 'D':  # Delete
            stock = session.query(StockPrice).filter_by(ticker=ticker, timestamp=timestamp).first()
            if stock:
                session.delete(stock)
        
        session.commit()

# Example incremental data (you can load this from CSV)
incremental_data = [
    {'ticker': 'Ticker-1', 'closing_value': 97, 'timestamp': '2023-04-02', 'action': 'I'},
    {'ticker': 'Ticker-2', 'closing_value': 102, 'timestamp': '2023-04-02', 'action': 'I'},
    {'ticker': 'Ticker-3', 'closing_value': 110, 'timestamp': '2023-04-01', 'action': 'U'},
]

process_incremental_data(incremental_data)
4. Plot Generation and Email Sending
Plotting the Data:
Use Matplotlib or Plotly to generate plots based on stock price data.
The plot should show the price trends for a particular stock or multiple stocks over time.
Email Service:
Use a service like Amazon SES, SendGrid, or SMTP to send the email containing the plots.
Include the plots as attachments or embedded images in the email.
Users should be able to subscribe to specific tickers. This information can be stored in a user-subscription table.
Subscription Management:
Store a user-subscription mapping in a table:

sql
Copy code
CREATE TABLE user_subscriptions (
  user_id INT,
  ticker VARCHAR(255),
  PRIMARY KEY (user_id, ticker)
);
Periodically (e.g., once a day after the data is processed), generate the plots and send the emails to users who are subscribed to those tickers.

Code for Email Sending (Python Example):
python
Copy code
import matplotlib.pyplot as plt
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.image import MIMEImage

def send_email(user_email, plot_image):
    msg = MIMEMultipart()
    msg['From'] = 'your-email@example.com'
    msg['To'] = user_email
    msg['Subject'] = 'Stock Price Trends for Today'

    body = MIMEText('Attached is the daily stock price trend plot.', 'plain')
    msg.attach(body)

    # Attach the plot image
    with open(plot_image, 'rb') as f:
        img = MIMEImage(f.read())
        msg.attach(img)

    server = smtplib.SMTP('smtp.example.com', 587)
    server.starttls()
    server.login('your-email@example.com', 'your-password')
    server.sendmail('your-email@example.com', user_email, msg.as_string())
    server.quit()

# Example of generating and sending a plot
def generate_plot(ticker_data):
    plt.plot(ticker_data['date'], ticker_data['closing_value'])
    plt.title('Stock Price Trend')
    plt.xlabel('Date')
    plt.ylabel('Price')

    # Save the plot to a file
    plot_image = 'stock_trend.png'
    plt.savefig(plot_image)

    # Send email to subscribers (user_email would be fetched from subscription DB)
    send_email('user@example.com', plot_image)

# Generate a plot for a sample ticker
sample_data = {'date': ['2023-01-01', '2023-02-01'], 'closing_value': [100, 110]}
generate_plot(sample_data)
5. Scalability and Reliability Considerations
Scalability:
If the data volume increases, consider sharding the database or using a distributed system like Google BigQuery, Amazon Redshift, or ClickHouse.
Use queues (e.g., RabbitMQ or Kafka) for handling data updates and email requests asynchronously.
Reliability:
Ensure error handling and retries for data processing failures.
Use a monitoring system (e.g., Prometheus, Grafana) to monitor pipeline health.
Conclusion
To build this service, we designed a pipeline that ingests, processes, and stores stock price data efficiently, generates stock price trend plots, and sends daily emails based on user subscriptions. We addressed















